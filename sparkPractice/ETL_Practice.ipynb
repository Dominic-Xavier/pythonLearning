{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/28 13:01:11 WARN Utils: Your hostname, dominic-To-be-filled-by-O-E-M resolves to a loopback address: 127.0.1.1; using 192.168.29.82 instead (on interface wlp2s0)\n",
      "25/06/28 13:01:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/28 13:01:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark_Hadoop\")\\\n",
    ".master(\"local[*]\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- heartrate: double (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n",
      "+---------------+-----------------+----------------------------+\n",
      "|device_id      |heartrate        |time                        |\n",
      "+---------------+-----------------+----------------------------+\n",
      "|161910         |68.64985536254405|2019-12-01T11:57:31.000+0000|\n",
      "|142838         |73.42208600878647|2019-12-01T11:58:03.000+0000|\n",
      "|124880         |75.97373066574369|2019-12-01T11:58:03.000+0000|\n",
      "|134009         |79.77338592716501|2019-12-01T11:58:20.000+0000|\n",
      "|NULL           |67.91460485184962|2019-12-01T11:58:33.000+0000|\n",
      "|109290         |NULL             |2019-12-01T11:58:33.000+0000|\n",
      "|193806         |-69.6489747665297|2019-12-01T11:58:34.000+0000|\n",
      "|Tracker #161910|65.3691488917709 |2019-12-01T11:58:42.000+0000|\n",
      "+---------------+-----------------+----------------------------+\n",
      "\n",
      "root\n",
      " |-- action: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- workout_id: long (nullable = true)\n",
      "\n",
      "+------+-------------+----------+-------+----------+\n",
      "|action|session_id   |timestamp |user_id|workout_id|\n",
      "+------+-------------+----------+-------+----------+\n",
      "|stop  |9            |1576482430|25477  |9         |\n",
      "|start |7            |1576501500|25143  |21        |\n",
      "|start |7            |1576505220|41732  |24        |\n",
      "|stop  |69           |1576506500|49296  |33        |\n",
      "|start |101          |1576507900|40872  |NULL      |\n",
      "|NULL  |508          |1576508670|16093  |27        |\n",
      "|stop  |FIRST_SESSION|1576508160|14633  |42        |\n",
      "+------+-------------+----------+-------+----------+\n",
      "\n",
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- street_address: string (nullable = true)\n",
      " |    |-- zip: long (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- update_type: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n",
      "+---------------------------------------------------------+----------+-----------+------+---------+---+----------+-----------+-------+\n",
      "|address                                                  |dob       |first_name |gender|last_name|sex|timestamp |update_type|user_id|\n",
      "+---------------------------------------------------------+----------+-----------+------+---------+---+----------+-----------+-------+\n",
      "|{Los Angeles, CA, 4791 Nathan Turnpike Suite 540, 90024} |02/01/2001|Shelley    |F     |Andrews  |F  |1555539580|new        |31362  |\n",
      "|{Los Angeles, CA, 4791 Nathan Turnpike Suite 540, 90024} |02/02/2001|Andrew     |F     |Shelley  |F  |1555539580|update     |31362  |\n",
      "|{Canyon Country, CA, 973 Norman Mountain Apt. 290, 91386}|05/10/1965|Jennifer   |F     |Perez    |F  |1559688700|new        |33987  |\n",
      "|{El Monte, CA, 696 Justin Crest Apt. 656, 91731}         |12/13/1933|Christopher|M     |Juarez   |M  |1565615100|new        |45875  |\n",
      "|{El Monte, CA, 696 Justin Crest Apt. 656, 91731}         |12/13/1933|Christopher|M     |Juarez   |M  |1565615100|new        |45875  |\n",
      "|{Woodland Hills, CA, 843 Hannah Corners, 91367}          |11/19/1944|Joshua     |M     |Mitchell |M  |1557990020|new        |NULL   |\n",
      "|{Glendale, CA, 203 Nicholson Mountains, 91201}           |01-05-1964|Sandra     |F     |Flores   |F  |1563474300|NULL       |41954  |\n",
      "|{Altadena, CA, 27548 Craig Dale Apt. 118, 91001}         |03/07/1941|Casey      |M     |Miles    |M  |1566185600|new        |#36644 |\n",
      "+---------------------------------------------------------+----------+-----------+------+---------+---+----------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from myData import test_data\n",
    "from io import StringIO\n",
    "data = test_data()\n",
    "\n",
    "# Read bpm_json from string\n",
    "bpm_df = spark.read.json(spark.sparkContext.parallelize(StringIO(data.bpm_json).readlines()))\n",
    "bpm_df.printSchema()\n",
    "bpm_df.show(truncate=False)\n",
    "\n",
    "workouts_df = spark.read.json(spark.sparkContext.parallelize(StringIO(data.workouts_json).readlines()))\n",
    "workouts_df.printSchema()\n",
    "workouts_df.show(truncate=False)\n",
    "\n",
    "users_cdc_df = spark.read.json(spark.sparkContext.parallelize(StringIO(data.users_cdc_json).readlines()))\n",
    "users_cdc_df.printSchema()\n",
    "users_cdc_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data using filter\n",
      "+---------+-----------------+----------------------------+\n",
      "|device_id|heartrate        |time                        |\n",
      "+---------+-----------------+----------------------------+\n",
      "|161910   |68.64985536254405|2019-12-01T11:57:31.000+0000|\n",
      "|142838   |73.42208600878647|2019-12-01T11:58:03.000+0000|\n",
      "|124880   |75.97373066574369|2019-12-01T11:58:03.000+0000|\n",
      "|134009   |79.77338592716501|2019-12-01T11:58:20.000+0000|\n",
      "|193806   |-69.6489747665297|2019-12-01T11:58:34.000+0000|\n",
      "+---------+-----------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"Cleaning data using filter\")\n",
    "cleanBPMData = bpm_df.filter(col(\"device_id\").isNotNull() & col(\"heartrate\").isNotNull() & \\\n",
    "                bpm_df.device_id.cast(\"long\").isNotNull())\n",
    "cleanBPMData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating average heartrate per device_id\n",
      "+---------+-----------------+\n",
      "|device_id|avg_heartrate    |\n",
      "+---------+-----------------+\n",
      "|124880   |75.97373066574369|\n",
      "|134009   |79.77338592716501|\n",
      "|142838   |73.42208600878647|\n",
      "|161910   |68.64985536254405|\n",
      "|193806   |-69.6489747665297|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "print(\"Calculating average heartrate per device_id\")\n",
    "cleanBPMData.groupBy(\"device_id\").agg(avg(\"heartrate\").alias(\"avg_heartrate\"))\\\n",
    "    .orderBy(\"device_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------+----------+----+------+\n",
      "|user_id|timestamp |readable_time      |date      |hour|minute|\n",
      "+-------+----------+-------------------+----------+----+------+\n",
      "|25477  |1576482430|2019-12-16 13:17:10|2019-12-16|13  |17    |\n",
      "|25143  |1576501500|2019-12-16 18:35:00|2019-12-16|18  |35    |\n",
      "|41732  |1576505220|2019-12-16 19:37:00|2019-12-16|19  |37    |\n",
      "|49296  |1576506500|2019-12-16 19:58:20|2019-12-16|19  |58    |\n",
      "|40872  |1576507900|2019-12-16 20:21:40|2019-12-16|20  |21    |\n",
      "|16093  |1576508670|2019-12-16 20:34:30|2019-12-16|20  |34    |\n",
      "|14633  |1576508160|2019-12-16 20:26:00|2019-12-16|20  |26    |\n",
      "+-------+----------+-------------------+----------+----+------+\n",
      "\n",
      "+-------+----------+-------------------+----------+----+------+\n",
      "|user_id|timestamp |readable_time      |date      |hour|minute|\n",
      "+-------+----------+-------------------+----------+----+------+\n",
      "|31362  |1555539580|2019-04-18 03:49:40|2019-04-18|3   |49    |\n",
      "|31362  |1555539580|2019-04-18 03:49:40|2019-04-18|3   |49    |\n",
      "|33987  |1559688700|2019-06-05 04:21:40|2019-06-05|4   |21    |\n",
      "|45875  |1565615100|2019-08-12 18:35:00|2019-08-12|18  |35    |\n",
      "|45875  |1565615100|2019-08-12 18:35:00|2019-08-12|18  |35    |\n",
      "|NULL   |1557990020|2019-05-16 12:30:20|2019-05-16|12  |30    |\n",
      "|41954  |1563474300|2019-07-18 23:55:00|2019-07-18|23  |55    |\n",
      "|#36644 |1566185600|2019-08-19 09:03:20|2019-08-19|9   |3     |\n",
      "+-------+----------+-------------------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime, to_date, date_format, hour, minute\n",
    "\n",
    "from myData import test_data\n",
    "data = test_data()\n",
    "\n",
    "workouts_df = workouts_df.withColumn(\"readable_time\", from_unixtime(\"timestamp\")) \\\n",
    "                         .withColumn(\"date\", to_date(\"readable_time\")) \\\n",
    "                         .withColumn(\"hour\", hour(\"readable_time\")) \\\n",
    "                         .withColumn(\"minute\", minute(\"readable_time\"))\n",
    "\n",
    "workouts_df = workouts_df.select(\"user_id\", \"timestamp\", \"readable_time\", \"date\", \"hour\", \"minute\")\n",
    "workouts_df.show(truncate=False)\n",
    "\n",
    "users_df = users_cdc_df.withColumn(\"readable_time\", from_unixtime(\"timestamp\")) \\\n",
    "                   .withColumn(\"date\", to_date(\"readable_time\")) \\\n",
    "                   .withColumn(\"hour\", hour(\"readable_time\")) \\\n",
    "                   .withColumn(\"minute\", minute(\"readable_time\"))\n",
    "\n",
    "users_df.select(\"user_id\", \"timestamp\", \"readable_time\", \"date\", \"hour\", \"minute\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|action|count|\n",
      "+------+-----+\n",
      "|NULL  |1    |\n",
      "|start |3    |\n",
      "|stop  |3    |\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "workouts_df.groupBy(\"action\").count().orderBy(\"action\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-------+----------+-------------------+----------+----+------+\n",
      "|action|session_id|timestamp |user_id|workout_id|readable_time      |date      |hour|minute|\n",
      "+------+----------+----------+-------+----------+-------------------+----------+----+------+\n",
      "|stop  |9         |1576482430|25477  |9         |2019-12-16 13:17:10|2019-12-16|13  |17    |\n",
      "|start |7         |1576501500|25143  |21        |2019-12-16 18:35:00|2019-12-16|18  |35    |\n",
      "|start |7         |1576505220|41732  |24        |2019-12-16 19:37:00|2019-12-16|19  |37    |\n",
      "|stop  |69        |1576506500|49296  |33        |2019-12-16 19:58:20|2019-12-16|19  |58    |\n",
      "|NULL  |508       |1576508670|16093  |27        |2019-12-16 20:34:30|2019-12-16|20  |34    |\n",
      "+------+----------+----------+-------+----------+-------------------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "workout_df = workouts_df.filter(col(\"workout_id\").isNotNull() & workouts_df.session_id.cast(\"long\").isNotNull())\n",
    "workout_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+----------+-----------+------+---------+---+----------+-----------+-------+-------------------+----------+----+------+\n",
      "|address                                                  |dob       |first_name |gender|last_name|sex|timestamp |update_type|user_id|readable_time      |date      |hour|minute|\n",
      "+---------------------------------------------------------+----------+-----------+------+---------+---+----------+-----------+-------+-------------------+----------+----+------+\n",
      "|{Los Angeles, CA, 4791 Nathan Turnpike Suite 540, 90024} |02/01/2001|Shelley    |F     |Andrews  |F  |1555539580|new        |31362  |2019-04-18 03:49:40|2019-04-18|3   |49    |\n",
      "|{Los Angeles, CA, 4791 Nathan Turnpike Suite 540, 90024} |02/02/2001|Andrew     |F     |Shelley  |F  |1555539580|update     |31362  |2019-04-18 03:49:40|2019-04-18|3   |49    |\n",
      "|{Canyon Country, CA, 973 Norman Mountain Apt. 290, 91386}|05/10/1965|Jennifer   |F     |Perez    |F  |1559688700|new        |33987  |2019-06-05 04:21:40|2019-06-05|4   |21    |\n",
      "|{El Monte, CA, 696 Justin Crest Apt. 656, 91731}         |12/13/1933|Christopher|M     |Juarez   |M  |1565615100|new        |45875  |2019-08-12 18:35:00|2019-08-12|18  |35    |\n",
      "|{El Monte, CA, 696 Justin Crest Apt. 656, 91731}         |12/13/1933|Christopher|M     |Juarez   |M  |1565615100|new        |45875  |2019-08-12 18:35:00|2019-08-12|18  |35    |\n",
      "|{Woodland Hills, CA, 843 Hannah Corners, 91367}          |11/19/1944|Joshua     |M     |Mitchell |M  |1557990020|new        |NULL   |2019-05-16 12:30:20|2019-05-16|12  |30    |\n",
      "|{Glendale, CA, 203 Nicholson Mountains, 91201}           |01-05-1964|Sandra     |F     |Flores   |F  |1563474300|NULL       |41954  |2019-07-18 23:55:00|2019-07-18|23  |55    |\n",
      "|{Altadena, CA, 27548 Craig Dale Apt. 118, 91001}         |03/07/1941|Casey      |M     |Miles    |M  |1566185600|new        |#36644 |2019-08-19 09:03:20|2019-08-19|9   |3     |\n",
      "+---------------------------------------------------------+----------+-----------+------+---------+---+----------+-----------+-------+-------------------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+----------+----------+------+---------+---+----------+-----------+-------+-------------------+----------+----+------+-----+\n",
      "|address                                                  |dob       |first_name|gender|last_name|sex|timestamp |update_type|user_id|readable_time      |date      |hour|minute|count|\n",
      "+---------------------------------------------------------+----------+----------+------+---------+---+----------+-----------+-------+-------------------+----------+----+------+-----+\n",
      "|{Los Angeles, CA, 4791 Nathan Turnpike Suite 540, 90024} |02/01/2001|Shelley   |F     |Andrews  |F  |1555539580|new        |31362  |2019-04-18 03:49:40|2019-04-18|3   |49    |1    |\n",
      "|{Los Angeles, CA, 4791 Nathan Turnpike Suite 540, 90024} |02/02/2001|Andrew    |F     |Shelley  |F  |1555539580|update     |31362  |2019-04-18 03:49:40|2019-04-18|3   |49    |1    |\n",
      "|{Canyon Country, CA, 973 Norman Mountain Apt. 290, 91386}|05/10/1965|Jennifer  |F     |Perez    |F  |1559688700|new        |33987  |2019-06-05 04:21:40|2019-06-05|4   |21    |1    |\n",
      "|{Altadena, CA, 27548 Craig Dale Apt. 118, 91001}         |03/07/1941|Casey     |M     |Miles    |M  |1566185600|new        |#36644 |2019-08-19 09:03:20|2019-08-19|9   |3     |1    |\n",
      "+---------------------------------------------------------+----------+----------+------+---------+---+----------+-----------+-------+-------------------+----------+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "usersRecord = users_df.groupBy(users_cdc_df.columns).agg(count(\"*\").alias(\"count\")).filter(col(\"count\") == 1) \\\n",
    ".filter(col(\"user_id\").isNotNull() & col(\"update_type\").isNotNull())\n",
    "usersRecord.show(truncate=False)\n",
    "\n",
    "usersRecord = users_df.groupBy(users_df.columns).agg(count(\"*\").alias(\"count\")) \\\n",
    "    .filter((col(\"count\") == 1) & col(\"user_id\").isNotNull() & col(\"update_type\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+------+---------+---+----------+-----------+-------+-------------------+----------+----+------+-----+----------+\n",
      "|             address|       dob|first_name|gender|last_name|sex| timestamp|update_type|user_id|      readable_time|      date|hour|minute|count|row_number|\n",
      "+--------------------+----------+----------+------+---------+---+----------+-----------+-------+-------------------+----------+----+------+-----+----------+\n",
      "|{Altadena, CA, 27...|03/07/1941|     Casey|     M|    Miles|  M|1566185600|        new| #36644|2019-08-19 09:03:20|2019-08-19|   9|     3|    1|         1|\n",
      "|{Los Angeles, CA,...|02/01/2001|   Shelley|     F|  Andrews|  F|1555539580|        new|  31362|2019-04-18 03:49:40|2019-04-18|   3|    49|    1|         1|\n",
      "|{Los Angeles, CA,...|02/02/2001|    Andrew|     F|  Shelley|  F|1555539580|     update|  31362|2019-04-18 03:49:40|2019-04-18|   3|    49|    1|         2|\n",
      "|{Canyon Country, ...|05/10/1965|  Jennifer|     F|    Perez|  F|1559688700|        new|  33987|2019-06-05 04:21:40|2019-06-05|   4|    21|    1|         1|\n",
      "+--------------------+----------+----------+------+---------+---+----------+-----------+-------+-------------------+----------+----+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc\n",
    "usersRecordFilter = usersRecord.withColumn(\"row_number\",\n",
    "    row_number().over(Window.partitionBy(\"user_id\").orderBy(desc(\"date\"))))\n",
    "\n",
    "usersRecord.withColumn(\"row_number\",\n",
    "    row_number().over(Window.partitionBy(\"user_id\").orderBy(desc(\"date\")))).collect()\n",
    "\n",
    "usersRecordFilter.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
